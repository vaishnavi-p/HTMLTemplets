<html>
<body>

<h1>Classification model evaluation</h1>


<ol>
  <li><b>Accuracy score:</b> 
<br>
The accuracy_score function computes the accuracy, either the
fraction (default) or the count of correct predictions.
If the entire set of predicted labels strictly match with the true set of labels, then
the subset accuracy is 1.0; otherwise it is 0.
</li>
  <li><b>Precision Score:</b><br>The precision is the ratio tp / (tp + fp) where tp is the number of
true positives and fp the number of false positives. The precision is intuitively the
ability of the classifier not to label as positive a sample that is negative.</li>
  <li><b>Recall Score:</b><br>
The recall is the ratio tp / (tp + fn) where tp is the number of true
positives and fn the number of false negatives. The recall is intuitively the ability
of the classifier to find all the positive samples.</li>
<li><b>F1 Score:</b><br>
The F1 score can be interpreted as a weighted average of the precision
and recall, where F1 score reaches its best value at 1 and worst score at 0.
</li>
<li>
<b>Classification report:</b><br>
Builds a report containing main classification metrics.
</li>
<li><b>Confusion matrix</b><br>
<script src="https://gist.github.com/sonalik23/a8fcb206b3a1d2647ac4a58947ceb047.js"></script>
</li>
</ol> 



</body>
</html>
